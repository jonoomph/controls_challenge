    PREVIOUS GAME LEVELS (including 80 initial steers)
    45: test	0.996	14.617	64.393
    50: test	0.987	14.686	64.018
    
    UPDATED GAME LEVELS: 00001.npy - 00006.npy:
    50: test	1.021	14.587	65.616
    
    UPDATED GAME LEVELS: 00120 - 00122:
    35: test    1.096	14.406	69.182
    
    UPDATED GAME LEVELS: 00123, 00124, 00159, 00254:
    35: test	1.096	14.406	69.182
    
    UPDATED GAME LEVELS: 00322, 00356, 00432:  *** EXPLODES MODEL
    35: test	12.312	16.892	632.476
    
    UPDATED GAME LEVELS: Reverting 00432:
    50: test	1.191	15.099	74.656
    
    UPDATED GAME LEVELS: Deleting 00432:
    35: test	1.525	14.532	90.791
    
    13 NEW GAME LEVELS: (equal distribution)
    30: test	1.317	13.932	79.772
    
    15 NEW GAME LEVELS: (equal distribution) 
    30: test	1.180	14.704	73.711 - ignore initial steer [80:]
    35: test	1.168	14.356	72.776 - with 80 initial steer

    27 NEW GAME LEVELS: (equal distribution) 
    20: test	1.235	14.621	76.391 - ignore initial steer [80:]
    45: test	1.144	15.202	72.409 - with 80 initial steer

    35 NEW GAME LEVELS: (equal distribution) 
    25: test	1.136	14.812	71.619 - ignore initial steer [80:]
    45: test	1.218	14.391	75.297 - with 80 initial steer
    
    39 NEW GAME LEVELS: (equal distribution) + 1 PID level
    20: test	1.059	14.936	67.875
    
    41 NEW GAME LEVELS: (equal distribution) + Many PID level
    45: test	1.144	14.474	71.688
    40: test    1.116	14.393	70.216 (no changes - just train again)
    
    CHANGE NN from 64 to 80 (no changes in data)
    40: test	1.019	14.771	65.712
    
    53 NEW GAME LEVELS: 
    20: test	1.088	14.478	68.882
    
    58 NEW GAME LEVELS: (deleted 4744, 900+ score)
    30: test	0.945	15.242	62.485
    40: test	0.928	15.453	61.843
    40: test	1.125	16.390	72.652 (5000 eval)
    
    CHANGE NN from 80 to 128 (includes 4744)
    25: test	0.977	15.707	64.548
    
    CHANGE NN to 96 (deleted 4744)
    20: test	1.028	15.082	66.482
    
    CHANGE NN back to 80 (set random seed: 42), still 58 game levels
    15: test	1.367	15.115	83.454
    20: test	1.560	15.529	93.540
    25: test	1.227	15.676	77.039
    30: test	1.396	15.966	85.776
    35: test	1.681	16.217	100.249
    
    NO CHANGES (set random seed: 1979)
    25: test	1.267	15.420	78.775
    30: test	1.207	15.981	76.311

    NO CHANGES (set random seed: 2000)
    25: test	1.248	15.336	77.750
    50: test	1.198	15.828	75.706

    NO CHANGES (set random seed: 2002, changed LR=0.00006)
    26: test	1.000	14.941	64.925

    NO CHANGES (set random seed: 962, changed LR=0.00006)
    25: test	0.992	14.764	64.375

    REMOVED LEVELS: 04743, 08335, 03166 (bad high scores)
    30: test	1.081	14.498	68.549  # slower to train

    ADDED LEVELS (1,2,3, 120, 121, 122) = got worse
    20: test	1.001	14.612	64.677

    REVERTED BACK TO 58 LEVELS (tensor_data[60:]) = got worse
    24: test	1.001	14.715	64.781

    UPDATE MODEL TO 64 NN (tensor_data[80:]) = got worse
    24: test	1.060	14.515	67.525

    UPDATE MODEL TO 80 NN
    25: test	0.992	14.764	64.375

    UPDATED GAME LEVELS to 63
    21: test	0.997	14.486	64.346

    UPDATED GAME LEVELS to 82 (removing 14278, 14400, 12758) = got worse  (more bad files: 08335, 04743, 03166)
    18: test	1.220	14.029	75.007

    TRAINING all 84 levels (analyzing which files are the worst over 20 epochs) - 10 worst files
    File                 Mean Loss       Epoch Count
    ---------------------------------------------
    1   ./simulations/03166.pth 0.743433        20
    2   ./simulations/04743.pth 0.458917        20
    3   ./simulations/14278.pth 0.442126        20
    4   ./simulations/00140.pth 0.265490        20
    5   ./simulations/01611.pth 0.263488        20
    6   ./simulations/02355.pth 0.244197        20
    7   ./simulations/07283.pth 0.217067        20
    8   ./simulations/14400.pth 0.216369        20
    9   ./simulations/00437.pth 0.199559        20
    10  ./simulations/08335.pth 0.183868        20
    11  ./simulations/04358.pth 0.167655        20

    REMOVED top 11 worst training files (73 levels remain)
    33: test	1.425	14.962	86.223

    SWITCHED loss function to SmoothL1Loss()
    33: test	1.440	14.912	86.916

    REVERTED. GOT G29 WHEEL. FIRST 7 WHEEL LEVELS (no PID)
    65: test	1.296	14.917	79.711  # still improving (worst files are high scores)

    IMPROVED 7 LEVELS (WHEEL ONLY) - Trained longer, 100 epochs
    83: test	1.186	15.063	74.386

    10 LEVELS (WHEEL ONLY)
    85: test	1.200	14.417	74.416

    File                 Mean Loss       Epoch Count
    ---------------------------------------------
    1   ./simulations/00140.pth 0.189727        100
    2   ./simulations/00437.pth 0.150982        100
    3   ./simulations/00367.pth 0.060024        100
    4   ./simulations/00669.pth 0.057937        100

    11 LEVELS (WHEEL ONLY) - updated 2 worst scores (got worse)
    61: test	1.255	13.962	76.721

    File                 Mean Loss       Epoch Count
    ---------------------------------------------
    1   ./simulations/00140.pth 0.168790        100
    2   ./simulations/00437.pth 0.145453        100
    3   ./simulations/00669.pth 0.062521        100
    4   ./simulations/00367.pth 0.060716        100

    11 LEVELS (WHEEL ONLY) - adding r2, slope to model (median with moving window) - model becomes very unstable after 75 epochs
    74: test	1.360	14.607	82.630

    27 LEVELS (WHEEL ONLY)
    23: test	1.045	14.524	66.753

    38 LEVELS (WHEEL ONLY)
    5: test	1.261	14.106	77.170

    File                 Mean Loss       Epoch Count
    ---------------------------------------------
    1   ./simulations/00140.pth 0.064160        100
    2   ./simulations/02355.pth 0.064128        100
    3   ./simulations/00437.pth 0.057320        100
    4   ./simulations/03273.pth 0.039570        100
    5   ./simulations/04358.pth 0.030827        100
    6   ./simulations/05216.pth 0.027430        100
    7   ./simulations/00367.pth 0.027366        100
    8   ./simulations/00669.pth 0.024083        100
    9   ./simulations/00236.pth 0.021043        100
    10  ./simulations/03975.pth 0.020578        100

    38 LEVELS (NO CHANGES) - lowering LR
    15: test	1.118	14.319	70.242

    File                 Mean Loss       Epoch Count
    ---------------------------------------------
    1   ./simulations/02355.pth 0.272035        40
    2   ./simulations/00140.pth 0.234221        40
    3   ./simulations/00437.pth 0.220579        40
    4   ./simulations/04358.pth 0.122256        40
    5   ./simulations/03273.pth 0.120247        40
    6   ./simulations/05679.pth 0.089220        40
    7   ./simulations/05216.pth 0.083075        40
    8   ./simulations/00367.pth 0.082475        40
    9   ./simulations/00669.pth 0.082144        40
    10  ./simulations/05324.pth 0.076274        40

    NO CHANGES - Changing NN to 96 AND 64 params
    X: BOTH BAD (96 worst performance, 64 also bad, 75 a little bad)
    40: test	1.077	15.159	69.033 - NN to 85

    NO CHANGES - NN to 85, input size = 25 (6 future values for all 4 categories) + previous action
    X: test	1.122	14.514	70.614

    Removed level 02355 (worst training loss)
    X: Got a little worse

    Added levels 51, 69 (worst scores in top 100)
    X: test	1.114	14.449	70.138

    ADDED DROPOUT (learning slower, more gradual, not as good scores)
    37: test	1.100	14.683	69.700 = dropout 0.3
    X:   = dropout 0.4 = identical to 0.3 dropout

    NO DROPOUT (lr=5.1039484000888e-05, seed=468075), Removed level 51, 29
    12: test	1.111	14.349	69.875
    12: test	1.368	14.559	82.937 (5000 tests)

    UPDATED scores on 5 levels (using arrows / road roll in game)
    12: test	1.116	14.418	70.207
    15: test	1.116	14.459	70.280  # update 2 more levels scores
    1   ./simulations/02355.pth 0.414758        25
    2   ./simulations/00140.pth 0.371897        25
    3   ./simulations/00437.pth 0.338049        25
    4   ./simulations/04358.pth 0.189470        25
    5   ./simulations/03273.pth 0.173253        25
    6   ./simulations/05679.pth 0.135399        25

    ADDED 8 new levels (new levels lowered score)
    25: test	0.981	15.242	64.276 (100 levels)
    30: test	0.950	15.312	62.812 (100 levels)
    30: test	1.092	16.669	71.246 (5000 levels)

    IMPROVED 10 high scores
    34: test	0.976	15.593	64.404 (100 levels, window size 22)
    37: test	0.951	15.717	63.261 (window size 30)
    37: test	0.947	15.689	63.030 (window size 20)

    REMOVING level 140 + ignoring windows > diff_threshold
    30: test	1.185	14.460	73.693 (diff .085, window size 20)
    30: test	0.942	15.541	62.646 (diff .1, window size 20)

    NO DIFFS - removed levels
    - removed 140 - got tiny bit better
    - removed 437 - got worse (added back)
    - removed 2355 - got tiny bit better: test	0.944	15.572	62.790
    - removed 7283 - got much worse (added back)
    - removed 6828 - got much worse (added back)
    - removed 7614 - got much worse (added back)
    - removed 4358 - got little worse (added back)

    1   ./simulations/00437.pth 0.098631        65
    2   ./simulations/07283.pth 0.092984        65
    3   ./simulations/06828.pth 0.070252        65
    4   ./simulations/07614.pth 0.065340        65
    5   ./simulations/04358.pth 0.060257        65
    6   ./simulations/03273.pth 0.054075        65
    7   ./simulations/07682.pth 0.049648        65

    DIFFS forward and backwards (0.03 to 1.0, 1.0 to 0.03)
    - No improvement
    - Linear (1.0 to 0.03), lr=0.000055: worse
    - Sigmoid curve (1.0 to 0.03), lr=0.000055, removed 8027: test	0.968	15.465	63.871
    - Sigmoid curve (0.03 to 1.0), lr=0.000055, removed 8027: test	0.968	15.465	63.871
    - No DIFF, lr=0.000055, removed 8027: test	0.969	15.284	63.718

    NO DIFFS, Adding 3 new files (1 by 1), updated 7682
    - Updated 7682: got much worse
    - Removed 7682: smoother cost curve, but a little worse (reverting back to un-updated version)
    - Added 7724, 7922, and 8027: Nice cost curve, but got a little worse
    - Added just 7724, 7922: Nice cost curve, but little worse: test	0.978	15.264	64.160

    CHANGE OPTIMIZER to AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)
    - terrible / broken

    SIMULATIONS
    - Random 20 files (seed 1979): 47: test	0.972	14.976	63.593
    - Update lr=5e-04: test	0.949	15.636	63.109
    - ONLY files 20% better than PID scores: test	0.968	15.188	63.603
    - ONLY files 15% better than PID scores: test	0.975	15.392	64.165
    - ONLY files 10% better than PID scores: test	0.923	15.556	61.714
    - ONLY files 5% better than PID scores: test	0.949	15.345	62.802

    ADDED 10 new levels
    - 60: test	0.956	15.765	63.576
    - Removed 08335: got worse

    File                 Mean Loss       Epoch Count
    ---------------------------------------------
    1   ./simulations/08335.pth 0.180724        60
    2   ./simulations/00437.pth 0.138862        60
    3   ./simulations/07283.pth 0.124840        60
    4   ./simulations/06828.pth 0.105372        60

    ADDED 7 MODEL OUTPUTS (STEER + 6 FUTURE DIFFS)
    25: test	0.916	16.920	62.724 (100 tests, but 5000 test score sucks: ~80)
    25: test	0.861	18.052	61.123 (added small torque adj in pid_model, based on diffs)
    25: test	0.823	18.036	59.177 (made torque adj dynamic: kP=.152)
    25: test	1.108	20.750	76.160 (approach did not scale to 5000 tests for some reason)
    25: test	1.100	19.067	74.062 (kP=.09, still didn't scale to 5000 tests)

    UPDATED MODEL NN to 100
    24: test	0.925	16.279	62.505 (torque adj dynamic: kP=.09)
    24: test	1.441	17.665	89.739 (5000 tests, bad score)

    OPTIMIZER for LEVEL DATA
    00002: Improved total cost from 95.93814630552095 to 72.61961958909242 (3 indexes in each direction, 1 iteration)
    00002: Improved total cost from 95.93814630552095 to 70.719230186088 (4 indexes in each direction, 1 iteration)
    Starting full optimizer run: 1:25 PM (34 minutes per file x 41 files == 25 hours)
    34: test	1.216	14.977	75.755 (8 optimized files, 100 tests, lr=0.0001)
    54: test	0.919	16.493	62.467 (25 optimized files, 100 tests, lr=0.0001)
    29: test	0.967	14.894	63.265 (31 optimized files, 100 tests, lr=0.00005)
    X:  (56 optimized files, 100 tests, lr=0.000085 - unstable LR)
    31: test	0.998	14.741	64.629 (56 optimized files, 100 tests, lr=0.00004)
    57: test	0.936	14.960	61.775 (56 optimized files, 100 tests, lr=0.00002)

    ADDING WORST LEVELS (in 100 tests)
    00033 00026, 00069, 00051, 00089
    40: test	0.931	14.650	61.205 (63 optimized files, lr=0.00002)
    80: test	0.910	14.772	60.295 (63 optimized files, lr=0.00001 - slow LR)
    60: test	0.931	14.730	61.291 (63 optimized files, lr=0.000015, 2nd optimization of lvl 26)

    UPDATE NN to 100, 128
    90: test	0.929	15.032	61.457 (NN 100, lr=0.00001 - slow to train)
    82: test	0.931	14.928	61.476 (NN 100, lr=0.0000125 - no real improvement)

    REOPTIMIZE 8335, 7283, NN back to 85 size, lr=00001
    100: test	0.921	14.918	60.980 (lr=0.00001 - slow to train)
    100: test	1.084	15.456	69.654 (5000 tests - best score yet)

    4 WORST SCORES (5000 tests): 01805.csv, 00633.csv, 04744.csv, 01716.csv, 00432.csv
    PLUS: 03001.csv solved with pid_top PID. ALL 6 new levels optimized multiple times.
    PLUS: Less future ranges: [(0, 1), (1, 3), (2, 5), (5, 9)], remove a_ego segments - 13 INPUTS
    X: test	0.846	14.877	57.163 (100 tests, lr=0.00001)
    X: test	0.938	15.586	62.500 (5000 tests, lr=0.00001 - NEW HIGH SCORE!!!!! +5.337)
    X: (lr=0.00001, added lataccel w/roll adjustment, back to 17 inputs - no improvement)
    53: test	0.794	15.589	55.293 (100 tests, lr=0.00001, less future ranges: [(0, 1), (1, 3), (2, 5)], +3 prev actions = 15 inputs)
    53: test	0.864	16.629	59.824 (5000 tests - NEW HIGH SCORE!!!!! +4.531)
    59: test	0.786	15.502	54.824 (100 tests)
    60: test	0.784	15.496	54.703 (100 tests)
    60: test	0.783	15.484	54.657 (100 tests, 34 window size)
    60: test	0.864	16.604	59.781 (5000 tests, 34 window size)

    MORE OPTIMIZED SIMULATIONS
    57: (14 optimized, 100 tests)
    57: (14 optimized, 5000 tests)

    MADE v_ego and a_ego not subtract current state, removed 04744
    - test	0.762	15.839	53.941 (100 tests)
    - test	0.853	16.708	59.366 (5000 tests, NEW HIGH SCORE!!!!! +5.425)

    OPTIMIZED 38 total files
    50: test	0.766	15.745	54.052 (100 tests - tiny improvement on 100 tests)

    PLOTTED MISSING LEVELS: 1883, 4874, 3277, 2546, 367, 1459, 3533, 1830, 2429, 4853, 739, 3281, 3727, 3546, 740, 2684, 1961, 319, 1793, 3873, 1654, 575, 387, 4649, 1487
    46: test	0.805	15.426	55.677 (10 new levels, 100 tests, a little worse on all tests)

    COMBINED SIMULATIONS (44 PID-REPLAY + 36 PID-TOP)
    45: test	0.693	16.309	50.954 (100 tests)
    46: test	0.690	16.319	50.830 (100 tests)
    46: test	0.747	17.737	55.089 (5000 tests, NEW HIGH SCORE!!!!! +4.259)

    MADE v_ego and a_ego not subtract current state (a small improvement)
    48: test	0.679	16.426	50.352 (100 tests)
    48: Average Cost: 50.7170 (200 tests - ran test_models on 200 tests)
    48: test	0.758	17.927	55.849 (5000 tests, +5.497)
    43: test	0.761	17.995	56.049 (5000 tests, lowest test score in 20 tests)

    MADE only v_ego absolute (reverted a_ego). ADDED PID-FF. PID-TOP: 24 wins, PID-REPLAY: 39 wins, PID-FF: 17 wins
    X: test	0.639	16.724	48.665 (100 tests, big improvement)
    X: test	0.722	18.520	54.641 (5000 tests, NEW HIGH SCORE!!!! +5.975)

    ADDED A FEW MISSING PLOT LEVELS: PID-TOP: 36 wins, PID-REPLAY: 40 wins, PID-FF: 24 wins
    32: test	0.619	17.150	48.092 (100 tests)
    33: test	0.619	17.107	48.048 (100 tests)

    ADDED 20 HIGHEST SCORES FROM CSV (PID-TOP: 40 wins, PID-REPLAY: 42 wins, PID-FF: 37 wins, PID: 1 wins), W/ 0.08 filtering
    45: test	0.630	17.083	48.559 (100 tests, test score plots slightly worse)
    45: test	0.682	18.372	52.459 (5000 tests, NEW HIGH SCORE!!! +3.9)
    60: test	0.633	17.055	48.705 (0.07 filtering, lower LR: 0.000009, 100 tests, slightly worse)
    42: test	0.624	17.101	48.303 (0.09 filtering, 100 tests, slightly better)
    42: test	0.652	17.717	50.301 (300 tests)
    50: test	0.699	17.930	52.862 (300 tests)
    42: test	0.689	18.688	53.142 (0.09 filtering, 5000 tests)

    UPDATING SAVE SIMULATIONS (ALSO SAVE CLOSE REPLAYS)
    WORST SCORES: 4959, 4744, 3166, 1722, 2531, 1193, 4803
    - 04959: PID : 2502.9 cost (PID-REPLAY: +2427.4, PID-TOP: +899.3, PID-FF: +1455.7, PID-FUTURE: +11481.2)
    - 04744: PID-REPLAY : 668.3 cost (PID-TOP: +1085.9, PID-FF: +4904.8, PID-FUTURE: +53361.2, PID: +3889.9)
    - 03166: PID-TOP : 565.7 cost (PID-REPLAY: +20787.1, PID-FF: +114.7, PID-FUTURE: +5845.3, PID: +1279.2)
    - 01722: PID-FF : 640.5 cost (PID-REPLAY: +11295.4, PID-TOP: +160.6, PID-FUTURE: +17406.9, PID: +2084.7)
    - 02531: PID-FF : 785.9 cost (PID-REPLAY: +13442.0, PID-TOP: +246.0, PID-FUTURE: +88002.3, PID: +3143.2)
    - 01193: PID-FF : 410.5 cost (PID-REPLAY: +9961.2, PID-TOP: +224.7, PID-FUTURE: +56317.8, PID: +1623.6)
    - 04803: PID-FF : 541.4 cost (PID-REPLAY: +8318.7, PID-TOP: +168.9, PID-FUTURE: +5655.7, PID: +1705.8)
    TOTAL LEVELS: 138 SIMULATIONS: PID-TOP: 39 wins, PID-REPLAY: 80 wins, PID-FF: 33 wins
    43: test	0.639	17.045	48.977 (100 tests, 0.075 filter)
    43: test	0.687	18.369	52.708 (5000 tests)
    24: test	0.642	16.754	48.865 (100 tests, no filter)
    24: test	0.743	18.676	55.819 (5000 tests, no filter, also with close replays)

    NO CLOSE REPLAYS (PID-TOP: 39 wins, PID-REPLAY: 54 wins, PID-FF: 33 wins)
    X: test	0.626	16.937	48.217 (100 tests, NO FILTER)
    X: test	0.706	18.602	53.882 (5000 tests, NO FILTER)

    ADDED COST TO EACH STEERING TORQUE (train on all data, but weight it based on cost)
    31: test	0.633	16.884	48.537 (100 tests, ABS + Clamp steer cost -1:1)
    31: test	0.706	18.480	53.765 (5000 tests, ABS + Clamp steer cost -1:1)
    29: test	0.625	16.890	48.118 (100 tests, Clamp -6:6 => 1 to 0)
    29: test	0.626	16.882	48.159 (100 tests, Clamp -2.5:2.5 => 1 to 0, smoother validation curve, slight improvement)
    29: test	0.626	16.865	48.154 (100 tests, Clamp -2.5:2.5 => 1 to 0, plus sigmoid)
    27: test	0.622	16.922	48.002 (100 tests, Clamp -2.5:0.5 => 1 to 0, plus sigmoid)
    27: test	0.623	17.292	48.419 (200 tests)
    27: test	0.698	18.509	53.407 (5000 tests)
    22: test	0.643	16.663	48.813 (100 tests, Clamp -2:0 => 1 to 0.5, no sigmoid, L1Loss)
    22: test	0.720	18.310	54.305 (5000 tests, Clamp -2:0 => 1 to 0.5, no sigmoid, L1Loss)
    X: (120 epoch training - unstable validation curve after epoch 80)

    ADDING DROPOUT to model (trying to address overfitting)
    34: test	0.688	16.179	50.598 (100 tests, dropout 0.2, no LSTM dropout, MSELoss, NN 64, unstable validation)
    X: (NN 80, dropout 0.4, no LSTM dropout, unstable validation)
    32: test	0.627	16.708	48.069 (100 tests, NN 80, dropout 0.0, no LSTM dropout)
    32: test	0.625	16.846	48.109 (200 tests)
    32: test	0.685	18.061	52.322 (5000 tests, NEW HIGH SCORE!!!!! +4.253)
    41: test	0.669	16.168	49.615 (100 tests, NN 80, dropout 0.25, no LSTM dropout - little worse)
    X: (100 tests, NN 80, dropout 0.35, no LSTM dropout - even worse)
    35: test	0.644	16.495	48.706 (100 tests, NN 80, dropout 0.1, no LSTM dropout - the less dropout, the better the score)
    35: test	0.641	16.618	48.665 (200 tests)
    35: test	0.710	17.885	53.406 (5000 tests)

    REMOVING 4744, WEIGHTED STEER COSTS: [0.028, 0.141, 0.831] in simulations (takes 3 time steps to realize cost)
    PID-TOP: 39 wins, PID-REPLAY: 46 wins, PID-FF: 33 wins
    33: test	0.623	16.651	47.818 (100 tests)
    33: test	0.622	16.812	47.907 (200 tests)
    33: test	0.688	18.110	52.499 (5000 tests)
    38: test	0.641	16.550	48.599 (100 tests)
    38: test	0.644	17.052	49.236 (200 tests)
    38: test	0.698	18.119	53.026 (5000 tests)

    ADDED 15 NEW LEVEL CANDIDATES FROM PID (< 100 score): PID-TOP: 50 wins, PID-REPLAY: 46 wins, PID-FF: 37 wins
    28: test	0.615	16.825	47.586 (100 tests)
    28: test	0.612	17.010	47.627 (200 tests)
    29: test	0.614	16.755	47.468 (100 tests)
    29: test	0.613	16.968	47.623 (200 tests)
    29: test	0.678	18.305	52.201 (5000 tests, NEW HIGH SCORE!!! +4.733)

    ADDED 20 NEW LEVEL CANDIDATES FROM PID (< 80 score), use subset [80:-80]:
    26: test	0.608	17.000	47.396 (100 tests)
    26: test	0.608	17.162	47.541 (200 tests)
    75: test	0.638	16.777	48.692 (100 tests)
    75: test	0.632	17.263	48.873 (200 tests)
    100: test	0.634	16.969	48.681 (100 tests)
    100: test	0.630	17.589	49.064 (200 tests)
    104: test	0.634	17.007	48.686 (100 tests)
    104: test	0.629	17.626	49.068 (200 tests)

    CLAMP & FILTER: filter seems unneeded and incompatible with clamp steer cost weighting
    X: (clamp -0.5:+0.5 => 1.0 to 0.0, not much change)
    28: test	0.638	17.002	48.90 (100 tests, clamp -1.5:+0.5 => 1.0 to 0.0, filter 0.08)
    28: test	0.661	16.842	49.882 (200 tests, clamp -1.5:+0.5 => 1.0 to 0.0, filter 0.08)

    ADDED 20 NEW LEVELS FROM PID (< 70 score), use subset [80:-80]
    28: test	0.605	16.930	47.168 (100 tests, clamp -3:+0.25 => 1.0 to 0.0, filter NONE, +20 files added by epoch 4)
    28: test	0.607	17.107	47.450 (200 tests)
    28: test	0.680	18.575	52.589 (5000 tests +5.421)
    22: test	0.615	17.003	47.747 (100 tests, clamped -2:1, filter NONE)
    22: test	0.620	17.214	48.201 (200 tests)
    21: test	0.618	17.030	47.922 (100 tests, clamped -2:2)
    21: test	0.620	17.199	48.219 (200 tests)

    ADDED 40 new levels < 60: NOTE: clamping positive steer costs smooths validation curve, lowers score
    17: test	0.617	17.053	47.896 (100 tests, clamped -4:4)
    17: test	0.618	17.243	48.139 (200 tests)
    17: test	0.701	18.827	53.853 (5000 tests, +7.957)
    X: (clamped -2:0.15, lowest 20 score: 42.9)
    X: (clamped -4:4, lowest 20 score: 43.1)
    X: (clamped -4:1, lowest 20 score: 42.6268)
    X: (clamped -4:1, filter mean steer cost <= 0.1, lowest 20 score: 44.4344, lr=2e-5)
    X: (clamped -4:1, filter std dev steer cost <= 0.7, lowest 20 score: 44.4344, lr=2e-5: std dev filtering makes validation worse)
    X: (clamped -2:2, filter abs mean < 2, lowest 20 score: 43.62, lr=2e-5)
    X: (clamped -4:0.5, no filters, lr=1.5e-5 - nice validation curve)
        16: test	0.603	17.012	47.173 (100 tests)
        16: test	0.608	17.171	47.552 (200 tests)
        16: test	0.674	18.599	52.281 (5000 tests, +5.108)

    PLAN:
    - revert back to lowest score dataset & verify
    - run 5000 simulation for top_pid, and ff_pid
    - compare with model pid output

    REVERTED MANY MISSING FILES, ADDED 30 NEW LEVEL CANDIDATES (cap 600 score)
    LEVELS: PID-TOP: 34 wins, PID-REPLAY: 47 wins, PID-FF: 34 wins
    X: test	0.594	17.172	46.856 (100 tests, clamped -4:0.5, no filters, lr=1.5e-5)
        X: test	0.591	17.276	46.839 (200 tests)
        X: test	0.672	18.954	52.570 (5000 tests, +5.714)

    ADDED 11 WORST TOTAL SCORE COMPARED TO PIDS 5000 eval (PID-TOP: 35 wins, PID-REPLAY: 47 wins, PID-FF: 40 wins, PID: 4 wins)
    X: test	0.591	17.105	46.647 (100 tests)
        X: test	0.594	17.308	46.992 (200 tests)
        X: test	0.658	18.846	51.756 (5000 tests, NEW HIGH SCORE!!!! +5.109, +3.676 points from 1st place)

    ADDED REPLAYS and OPTIMIZED 15 levels (from the previous candidates): PID-TOP: 33 wins, PID-REPLAY: 65 wins, PID-FF: 35 wins, PID: 3 wins
    28: test	0.599	17.096	47.057 (100 tests, with stability bonus 0.5)
        28: test	0.597	17.267	47.103 (200 tests)
        28: test	0.670	18.780	52.268 (5000 tests)
    26: test	0.601	17.153	47.183 (100 tests, with no stability bonus)
        26: test	0.598	17.282	47.207 (200 tests)
        26: test	0.684	18.941	53.156 (5000 tests)
    NOTE: Seems like either human replays OR optimizing has made model dumber

    REMOVING OPTIMIZATIONS - leaving human replay for the 15 levels: PID-REPLAY: 64 wins, PID-FF: 37 wins, PID-TOP: 33 wins, PID: 6 wins
    X: test	0.601	17.153	47.183 (100 tests, with no stability bonus)

    ADDING 3 STEER OUTPUTS
    25: test	0.568	18.191	46.576 (100 tests, only output 0)
        25: test	0.581	18.357	47.394 (200 tests)
    25: test	0.569	18.119	46.580 (100 tests, average of 3 outputs)
        25: test	0.578	18.271	47.152 (200 tests)

    REVERT BACK before replays and optimizing (but model still has 3 steer outputs)
    22: test	0.588	18.114	47.518 (100 tests)
    23: test	0.583	18.220	47.365 (100 tests)
    24: test	0.576	18.253	47.050 (100 tests)
    25: test	0.568	18.191	46.576 (100 tests)
        25: test	0.581	18.357	47.394 (200 tests)
    26: test	0.567	18.188	46.548 (100 tests)
        26: test	0.577	18.421	47.280 (200 tests)

    REVERT AGAIN (no new replays or optimizations), ADDED pid_model to SIMULATION DATA (EXPERIMENT). Still 3 steer outputs.
    MIGHT BLOW UP MODEL: PID-TOP: 18 wins, PID-MODEL: 48 wins, PID-REPLAY: 32 wins, PID-FF: 24 wins, PID: 4 wins
    NOTE: 20 tests no longer represents the same curve as 100 tests
    25: test	0.555	18.382	46.120 (100 tests)
    26: test	0.554	18.453	46.141 (100 tests)
        26: test	0.579	18.648	47.597 (200 tests)
    27: test	0.549	18.461	45.893 (100 tests)
        27: test	0.577	18.732	47.571  (200 tests)
    28: test	0.548	18.499	45.909 (100 tests)
        28: test	0.575	18.751	47.512 (200 tests)
        28: test	0.574	18.737	47.454 (200 tests, linear scaling between t1, t2, t3)
        28: test	0.650	20.677	53.168 (5000 tests, t0 output)
    29: test	0.546	18.528	45.823 (100 tests)
        29: test	0.578	18.850	47.758 (200 tests)
    30: test	0.547	18.500	45.840 (100 tests)
    31: test	0.545	18.412	45.670 (100 tests)
        31: test	0.578	18.773	47.650 (200 tests)
    32: test	0.548	18.488	45.905 (100 tests)
    33: test	0.547	18.477	45.806 (100 tests)
    34: test	0.546	18.446	45.770
        34: test	0.580	18.776	47.766 (200 tests)
    35: test	0.548	18.424	45.841

    REVERT AGAIN (no new replays or optimizations), REVERTING TO 1 steer OUTPUT
    REPRODUCE HIGH SCORE, USE MODEL TO RE-SAVE SIMULATIONS, TRAIN AGAIN
    27: test	0.599	17.225	47.186 (100 tests)
    28: test	0.598	17.191	47.071 (100 tests)
    29: test	0.592	17.142	46.734 (100 tests)
    30: test	0.592	17.192	46.809 (100 tests)
    31: test	0.593	17.120	46.749 (100 tests)
    32: test	0.590	17.100	46.597 (100 tests)
        32: test	0.593	17.298	46.933 (200 tests)
        32: test	0.658	18.847	51.764 (5000 tests)
    33: test	0.595	17.126	46.880 (100 tests)
    34: test	0.593	17.098	46.724 (100 tests)
    35: test	0.594	17.062	46.754 (100 tests)

    SAVED GOOD MODEL (51.764 score) /good/ folder, 1 steer output
    PID-TOP: 11 wins, PID-MODEL: 57 wins, PID-REPLAY: 28 wins, PID-FF: 27 wins, PID: 3 wins
    24: test	0.609	17.282	47.735 (100 tests)
    25: test	0.607	17.305	47.666 (100 tests)
    26: test	0.604	17.246	47.426 (100 tests)
    27: test	0.603	17.266	47.402 (100 tests)
    28: test	0.603	17.234	47.368 (100 tests)
    29: test	0.602	17.223	47.334 (100 tests)
        29: test	0.602	17.402	47.491 (200 tests)
    30: test	0.604	17.226	47.423 (100 tests)
    31: test	0.605	17.228	47.474 (100 tests)
    32: test	0.603	17.189	47.339 (100 tests)
        32: test	0.602	17.343	47.456 (200 tests)
        32: test	0.673	18.968	52.595 (5000 tests, got worse with model simulations)

    REVERT AGAIN - NO PID_MODEL SIMULATIONS, ADDED 60 NEW CANDIDATES (PID's only)
    155 simulations: PID-TOP: 60 wins, PID-REPLAY: 51 wins, PID-FF: 73 wins, PID: 5 wins, PID-FUTURE: 1 wins
        21: test	0.588	17.572	46.966 (200 tests)
    22: test	0.588	17.294	46.703 (100 tests)
        22: test	0.585	17.534	46.765 (200 tests)
        22: test	0.662	19.129	52.237 (5000 tests)
    23: test	0.584	17.308	46.507 (100 tests)
        23: test	0.586	17.699	46.996 (200 tests)
        23: test	0.661	19.138	52.177 (5000 tests)
    24: test	0.586	17.275	46.582 (100 tests)
        24: test	0.589	17.731	47.191 (200 tests)
        24: test	0.660	19.128	52.144 (5000 tests)
    25: test	0.584	17.209	46.432 (100 tests)
        25: test	0.590	17.694	47.194 (200 tests)
        25: test	0.650	19.015	51.521 (5000 tests, NEW HIGH SCORE!!! +5.089)
    26: test	0.585	17.238	46.512 (100 tests)
        26: test	0.590	17.743	47.253 (200 tests)
        26: test	0.650	19.023	51.503 (5000 tests, NEW HIGH SCORE!!! +4.991)
    27: test	0.587	17.240	46.575 (100 tests)
        27: test	0.649	18.978	51.450 (5000 tests, NEW HIGH SCORE!!! +4.875)
    28: test	0.591	17.207	46.736 (100 tests)
        28: test	0.650	18.974	51.479 (5000 tests, +4.743)
    32: test	0.601	17.158	47.215 (100 tests)
        32: test	0.657	18.956	51.826 (5000 tests, aiming for a +3.9 diff, +4.611)

    NEW PLAN - interpolate clamp range from -6:+6 to -4:0 over 15 EPOCHS (allow ALL data to train at first, then clamp down on only GOOD steers)
    20: test	0.603	17.190	47.341 (100 tests)
        20: test	0.597	17.338	47.187  (200 tests)
    25: test	0.597	17.294	47.129 (100 tests)
        25: test	0.597	17.634	47.481 (200 tests)

    NEW VALIDATION SET (150 candidate files)
    16: test	0.650	17.031	49.536 (100 tests)
    16: test	0.656	17.056	49.866 (200 tests)
    26: test	0.817	17.010	57.852 (100 tests)

    NEW VALIDATION SET (150 with no dupes - best score 112, much too high)
    28: test	0.616	17.118	47.941 (100 tests)
    28: test	0.619	17.831	48.803 (200 tests)
    28: test	0.673	18.934	52.572 (5000 tests)

    REVERT CHANGES
    18: test	0.674	19.060	52.756 (5000 tests)
    25: test	0.655	18.900	51.660 (5000 tests)

    ADDED 57 candidates (balanced on score - updated plot_levels)
    UPDATED 57 TESTING FILES (using plot_levels - trying to predict best model for 5000 sims)
    18: test	0.599	17.299	47.229 (100 tests)
    18: test	0.606	17.569	47.856 (200 tests)
    18: test	0.665	18.948	52.198 (5000 tests)
    25: test	0.621	17.087	48.150 (100 tests)
    25: test	0.617	17.359	48.201 (200 tests)
    25: test	0.678	18.796	52.690 (5000 tests)
    22: test	0.610	17.101	47.611 (100 tests)
    22: test	0.607	17.323	47.676 (200 tests)
    22: test	0.671	18.828	52.376 (5000 tests)

    REVERT CHANGES: Added 74 new candidates (plot levels)
    UPDATED 74 TESTING FILES (plot levels)
    16: test	0.602	17.229	47.333  (100 tests)
    16: test	0.664	18.795	51.976 (5000 tests)
    47: test	0.622	17.282	48.359 (100 tests)
    47: test	0.654	18.757	51.432 (5000 tests, NEW HIGH SCORE!!! +3.073)
    54: test	0.623	17.328	48.483 (100 tests)
    54: test	0.653	18.843	51.471 (5000 tests)

    EXPANDED BINS IN PLOT LEVELS: [1, 10, 20, 30, 40, 50, 60, 80, 100, 500],
    NEW INPUT RANGES: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)] + 5 PREV STEERS
    REPLACED ADDED LEVELS: +26,  REPLACED TESTS: +26
    15: test	0.642	17.263	49.378 (200 tests)
    17: test	0.641	17.114	49.183 (200 tests)

    NEW INPUT RANGES: [(0, 1), (1, 2), (2, 3), (3, 4)] + 4 PREV STEERS
    ADDED 11 NEW LEVELS (where PID MODEL under performs PIDs)
    23: test	0.591	17.611	47.138 (100 tests)
    23: test	0.591	17.957	47.502 (200 tests)
    23: test	0.643	19.263	51.416 (5000 tests, NEW HIGH SCORE!!!! +4.278)
    36: test	0.614	17.727	48.422 (200 tests)
    WINDOW_SIZE from 34 to 30
    23: test	0.590	17.607	47.105 (100 tests)
    23: test	0.587	17.914	47.266 (200 tests)
    23: test	0.642	19.262	51.384 (5000 tests, NEW HIGH SCORE!!!! +4.279)

    ADDED 25 worse controller candidates
    21: test	0.597	17.898	47.733 (200 tests)
    22: test	0.603	17.560	47.726 (100 tests)
    22: test	0.596	17.838	47.627 (200 tests)
    22: test	0.648	19.146	51.561 (5000 tests)

    REVERTING 25 worst controller candidates
    NEW INPUT RANGE: [(0, 1), (1, 2), (2, 3)] + 3 PREV STEERS
    25: test	0.564	18.522	46.703 (100 tests)
    25: test	0.565	19.110	47.377 (200 tests)
    25: test	0.637	20.528	52.380 (5000 tests)
    31: test	0.587	18.260	47.598 (100 tests)
    31: test	0.582	18.983	48.080 (200 tests)
    31: test	0.637	20.222	52.071 (5000 tests)

    NEW INPUT RANGE: [(1, 2), (2, 3), (3, 4)] + 3 PREV STEERS
    PID-EXPERIMENTAL: 40 wins, PID-REPLAY: 54 wins, PID-TOP: 23 wins, PID-FF: 47 wins, PID: 6 wins, PID-FUTURE: 1 wins

    32: test	0.558	17.522	45.413 (100 tests)
    32: test	0.554	17.727	45.425 (200 tests)
    32: test	0.608	19.065	49.446 (5000 tests, NEW HIGH SCORE!!!! +4.033)
    31: test	0.559	17.593	45.520 (100 tests)
    31: test	0.555	17.777	45.538 (200 tests)
    31: test	0.608	19.082	49.494 (5000 tests, +3.974)
    35: test	0.562	17.470	45.586 (100 tests)
    35: test	0.558	17.712	45.593 (200 tests)
    35: test	0.609	19.043	49.515 (5000 tests, +3.929)
    30: test	0.557	17.579	45.434 (100 tests)
    30: test	0.554	17.780	45.490 (200 tests)
    30: test	0.610	19.102	49.580 (5000 tests, +4.146)

    RECORDING 11 HUMAN REPLAYS, OPTIMIZING 6 of them, DELETING 3 of them (zig zag garbage)
    NEW TESTS (60 plot levels)
    33: test	0.560	17.389	45.402 (100 tests)
    33: test	0.560	17.696	45.672 (200 tests)
    33: test	0.612	18.950	49.527 (5000 tests, +4.125)
    34: test	0.561	17.340	45.394 (100 tests)
    34: test	0.560	17.717	45.707 (200 tests)
    34: test	0.611	18.936	49.506 (5000 tests)
    35: test	0.613	18.908	49.565 (5000 tests)
    36: test	0.614	18.878	49.583 (5000 tests)

    REMOVING 13 worst files (when only training on single file, 10 epochs, >1400 sim score)
    30: test	0.555	17.413	45.157 (100 tests)
    30: test	0.559	17.775	45.700 (200 tests)
    30: test	0.618	18.922	49.809 (5000 tests, did worse)

    REVERTING 13 deleted files
    29: test	0.560	17.587	45.595 (100 tests)
    29: test	0.561	17.857	45.892 (200 tests)
    29: test	0.619	19.176	50.102 (5000 tests)

    DELETE ALL "REPLAY" FILES
    X: Worse

    REVERTING ALL CHANGES (155 sims)
    27: test	0.560	17.638	45.630 (100 tests)
    31: test	0.561	17.395	45.428 (100 tests)
    31: test	0.559	17.779	45.715 (200 tests)
    31: test	0.612	18.950	49.571 (5000 tests)
    37: test	0.578	17.356	46.240 (100 tests)
    37: test	0.569	17.586	46.030 (200 tests)

    OPTUNA TRIAL 28 (81 files, LR=1e-4):
    10: test	0.565	17.667	45.931 (100)

    OPTUNA TRIAL 30 (85 files, LR=5e-5):
    19: test	0.563	17.454	45.594 (100)
    19: test	0.566	17.856	46.168 (200 tests)
    19: test	0.632	19.241	50.820 (5000 tests)

    REVERT ALL CHANGES (156 sims), MODEL (12 inputs + 1 prev steer), 3 outputs (steers)
    validation curve worse
    MODEL (15 inputs w/ 3 prev steer), 3 outputs (steers), 85 NN
    36: test	0.603	16.970	47.095 (100 tests)
    36: test	0.588	17.302	46.705 (200 tests)
    36: test	0.632	18.493	50.070 (5000 tests)
    MODEL (15 inputs w/ 3 prev steer), 2 outputs (steers), 85 NN, LR=1.5e-5
    27: test	0.542	18.252	45.330 (100 tests)
    27: test	0.589	19.919	49.364 (5000 tests, NEW HIGH SCORE!!! +4.034)

    NN 80, 3 HIDDEN LAYERS, worse than 2 layers, unstable
    23: test	0.575	18.410	47.184 (100 tests)
    23: test	0.578	18.613	47.530 (200 tests)

    NN 80, 2 HIDDEN LAYERS, Added 26 plotted controller under performers
    28: test	0.536	18.025	44.821 (100 tests)
    28: test	0.540	18.152	45.167 (200 tests)
    28: test	0.580	19.504	48.500 (5000 tests, NEW HIGH SCORE!!!! +3.679)

    ADDED 27 new under performing controller levels
    19: test	0.533	18.355	45.020 (100 tests)
    19: test	0.531	18.379	44.919 (200 tests)
    19: st	0.580	19.811	48.788 (5000 tests, got worse)

    REVERTED 27 new levels, NN 85, Removed 04803, 02531, 01387, 04731, clamp to -4:0.25
    29: test	0.525	18.548	44.815 (100)
    ADDING BACK 4 removed files
    26: worse - I think clamp 0.25 is the issue

    NN to 80, clamp to -5:1
    REMOVED 01611, 03166, 02531, 01387, 02931
    21: test	0.537	18.143	44.982 (100 tests)
    21: test	0.541	18.187	45.221 (200 tests)
    24: test	0.536	17.901	44.698 (100 tests)
    24: test	0.541	17.988	45.059 (200 tests)
    24: test	0.590	19.445	48.935 (5000 tests)

    REVERTING DELETED FILES, clamp to -4:1
    26: test	0.532	18.069	44.688 (100)
    26: test	0.535	18.224	44.998 (200)
    26: test	0.577	19.615	48.475 (5000, NEW HIGH SCORE!!! +3.787)

    CLAMP to -5:1
    X: seemed worse

    Trial 4: {'window_size': 26, 'hidden_size': 88, 'steer_clamp_min': -4.082161344429014, 'steer_clamp_max': 1.0197646935977005}
    X: worse
    Custom Trial: window_size=30, hidden_size=80, clamp_min=-4.5, clamp_max=0.5
    29: test	0.535	18.207	44.942 (100)
    29: test	0.533	18.337	44.977 (200)
    29: test	0.581	19.801	48.852 (5000)

    Trial 16: {'window_size': 32, 'hidden_size': 97, 'steer_clamp_min': -4.510828271339817, 'steer_clamp_max': 5.285405794708554}
    18: test	0.531	18.126	44.689 (100)
    18: test	0.538	18.289	45.173 (200, worse)

    Trial 36: Best trial parameters: {'batch_size': 33, 'window_size': 41, 'hidden_size': 99
    13: test	0.555	17.836	45.601 (100, worse on all epochs)

    Custom Trial:  batch_size=50, window_size=50, hidden_size=85, clamp_min=-4, clamp_max=1.0
    X: worse on all epochs

    Trial 2 finished with value: 48.776776926369166 and parameters: {'batch_size': 43, 'window_size': 34, 'hidden_size': 102}. Best is trial 2 with value: 48.776776926369166.
    19: test	0.539	18.307	45.268 (100)
    19: test	0.546	18.559	45.859 (200)
    19: test	0.599	20.071	50.013 (5000)

    NN to 80
    18: test	0.542	18.514	45.597 (100)
    25: test	0.533	18.072	44.708 (100)
    26: test	0.532	18.026	44.618 (100)
    26: test	0.535	18.206	44.949 (200)
    26: test	0.578	19.546	48.451 (5000, NEW HIGH SCORE!!!! +3.833)
    27: test	0.533	17.952	44.589 (100)
    27: test	0.537	18.129	44.994 (200)
    28: test	0.538	17.967	44.857 (100)

    NN 80, batch_size=30, window_size=40
    15: test	0.536	18.123	44.921 (100)
    16: test	0.535	18.083	44.826 (100)
    17: test	0.536	18.023	44.815 (100)
    17: test	0.547	18.175	45.541 (200)
    18: test	0.540	18.014	44.997 (100)
    19: test	0.543	17.925	45.099 (100)

    NN 90, batch_size=30, window_size=45, hidden_size=90
    X: worse

    NN 78, batch_size=40, window_size=35, hidden_size=78
    REMOVED 0633 level
    19: test	0.557	18.062	45.925 (100)
    15: test	0.564	18.302	46.484 (100)
    21: test	0.558	17.901	45.799 (100)
    23: test	0.560	17.779	45.795 (100)
    25: test	0.569	17.697	46.142 (100)

    NN 82, batch_size=35, window_size=40, hidden_size=82, clamp_min=-4, clamp_max=1.0)
    REMOVED 0633 level
    X: worse

    NN 82, batch_size=35, window_size=35, hidden_size=82, clamp_min=-4, clamp_max=1.0)
    ADDED BACK 0633 level
    21: test	0.549	17.890	45.365 (100)
    22: test	0.551	17.767	45.329 (100)
    23: test	0.554	17.750	45.468 (100)
    24: test	0.558	17.693	45.576 (100)

    NN to 84, batch_size=43, window_size=33, hidden_size=84, clamp_min=-4, clamp_max=1.0)
    23: test	0.562	18.140	46.219 (100)
    27: test	0.569	17.923	46.385 (100)
    34: test	0.590	17.616	47.106 (100)

    NN to 81, batch_size=40, window_size=33, hidden_size=81, clamp_min=-4, clamp_max=1.0)
    19: test	0.544	18.313	45.504 (100)
    22: test	0.545	18.187	45.440 (100)
    26: test	0.557	18.091	45.921 (100)

    NN to 80, batch_size=48, window_size=35, hidden_size=80, clamp_min=-4, clamp_max=1.0)
    BATCH SIZE >= Smoother Validation Curve
    20: test	0.542	18.605	45.693 (100)
    25: test	0.539	18.256	45.205 (100)
    23: test	0.538	18.424	45.308 (100)
    27: test	0.543	18.215	45.370 (100)

    NN to 80, batch_size=40, window_size=35, hidden_size=80, clamp_min=-4, clamp_max=1.0)
    24: test	0.539	18.212	45.176 (100)
    25: test	0.535	18.073	44.807 (100)
    26: test	0.538	18.017	44.918 (100)
    27: test	0.539	17.960	44.907 (100)
    29: test	0.546	17.887	45.176 (100)

    NN to 80, batch_size=44, window_size=35, hidden_size=80, clamp_min=-4, clamp_max=1.0)
    ADDED 4 new BAD SCORES - got worse
    17: test	0.547	18.697	46.024 (100)
    22: test	0.535	18.638	45.365 (100)
    24: test	0.534	18.540	45.261 (100)
    26: test	0.536	18.447	45.259 (100)
    28: test	0.541	18.363	45.401 (100)
    33: test	0.565	18.182	46.422 (100)

    REVERT CHANGES. NN to 80, batch_size=44, window_size=30, hidden_size=80, clamp_min=-5, clamp_max=1.5)
    lr=1.4e-5
    25: test	0.532	18.169	44.774 (100)
    26: test	0.532	18.101	44.705 (100)
    26: test	0.582	19.655	48.759 (5000)
    27: test	0.535	18.083	44.815 (100)
    28: test	0.537	17.970	44.821 (100)
    29: test	0.541	17.931	44.978 (100)
    29: test	0.589	19.501	48.970 (5000)
    30: test	0.546	17.888	45.202 (100)
    31: test	0.550	17.837	45.327 (100)

    NN to 80, batch_size=44, window_size=30, hidden_size=80, clamp_min=-4, clamp_max=1), lr=1.4e-5
    ADDING 27 under performing controller levels
    14: test	0.556	18.648	46.428 (100)
    14: test	0.557	18.693	46.550 (200)
    20: test	0.536	18.333	45.154 (100)
    20: test	0.534	18.366	45.077 (200)
    21: test	0.533	18.206	44.846 (100)
    22: test	0.532	18.113	44.691 (100)
    23: test	0.532	18.102	44.706 (100)
    24: test	0.532	17.990	44.600 (100)
    25: test	0.533	17.883	44.553 (100)
    25: test	0.538	18.017	44.898 (200)
    25: test	0.579	19.388	48.330 (5000, NEW HIGH SCORE!!!! +3.777)
    26: test	0.539	17.876	44.846 (100)
    27: test	0.543	17.800	44.954 (100)
    28: test	0.545	17.741	45.012 (100)

    ADDING 27 MORE under performing controller levels
    16: test	0.539	18.294	45.253 (100)
    17: test	0.534	18.174	44.864 (100)
    18: test	0.534	18.102	44.824 (100)
    19: test	0.532	18.029	44.605 (100)
    20: test	0.534	17.961	44.643 (100)
    21: test	0.534	17.882	44.590 (100)
    22: test	0.537	17.908	44.764 (100)
    23: test	0.537	17.796	44.671 (100)
    24: test	0.542	17.717	44.839 (100)
    25: test	0.546	17.668	44.954 (100)
    26: test	0.551	17.620	45.151 (100)
    27: test	0.557	17.553	45.413 (100)

    NN 84, batch_size=44, window_size=33, hidden_size=84, clamp_min=-4, clamp_max=1)
    Got worse, maybe due to unneeded extra window size, or larger NN causes memorizing noise
    14: test	0.560	19.141	47.155 (100)
    17: test	0.571	18.957	47.495 (100)
    21: test	0.614	18.574	49.259 (100)
    25: test	0.646	18.388	50.685 (100)

    NN 79, batch_size=44, window_size=25, hidden_size=79, clamp_min=-4, clamp_max=1)
    Trying to shrink both NN size and window size (just a bit)
    10: test	0.596	18.274	48.062 (100)
    19: test	0.566	17.893	46.176 (100)
    20: test	0.579	17.854	46.811 (100)

    REVERT NN back to 80, batch_size=44, window_size=30, hidden_size=80, clamp_min=-4, clamp_max=1)
    CHANGE INPUT (a_ego to absolute), RECORD NEW REPLAYS & OPTIMIZE
    23: test	0.524	18.025	44.241 (100)
    23: test	0.535	18.128	44.860 (200)
    23: test	0.572	19.446	48.048 (5000)
    24: test	0.531	18.056	44.607 (100)
    24: test	0.539	18.071	45.024 (200)
    24: test	0.572	19.376	47.983 (5000, NEW HIGH SCORE!!! +3.376 - RANK #1)
    25: test	0.530	17.941	44.460 (100)
    25: test	0.541	18.024	45.091 (200)
    25: test	0.577	19.327	48.189 (5000, +3.729)
    26: test	0.539	17.935	44.878 (100)
    26: test	0.546	17.961	45.264 (200)
    26: test	0.581	19.271	48.339 (5000)
    27: test	0.541	17.828	44.882 (100)
    28: test	0.546	17.779	45.069 (100)
    29: test	0.551	17.716	45.241 (100)
    30: test	0.556	17.683	45.473 (100)

    BEFORE REPLAY LEVELS & LITE OPTIMIZING
    PID-EXPERIMENTAL: 53 wins
    PID-REPLAY: 57 wins
    PID-TOP: 60 wins
    PID-FF: 72 wins
    PID: 6 wins
    PID-FUTURE: 1 wins

    AFTER
    PID-TOP: 58 wins
    PID-EXPERIMENTAL: 50 wins
    PID-REPLAY: 68 wins
    PID-FF: 70 wins
    PID: 5 wins
    PID-FUTURE: 1 wins

    AFTER ANOTHER ROUND OF REPLAYS
    PID-REPLAY: 74 wins
    PID-EXPERIMENTAL: 47 wins
    PID-TOP: 55 wins
    PID-FF: 69 wins
    PID: 5 wins
    PID-FUTURE: 1 wins

    MORE REPLAYS and OPTIMIZATIONS (removed level 522)
    23: test	0.525	18.074	44.320 (100)
    23: test	0.576	19.593	48.399 (5000)
    24: test	0.526	17.980	44.284 (100)
    24: test	0.577	19.520	48.379 (5000, got worse)
    25: test	0.529	17.932	44.396 (100)
    25: test	0.584	19.494	48.703 (5000)

    ADDED back level 522 (all worse)
    19: test	0.514	18.304	43.992 (100)
    20: test	0.515	18.198	43.946 (100)
    20: test	0.579	19.847	48.811 (5000)
    21: test	0.515	18.170	43.939 (100)
    22: test	0.519	18.080	44.006 (100)
    22: test	0.576	19.697	48.512 (5000)
    23: test	0.522	18.031	44.148 (100)
    24: test	0.529	18.011	44.451 (100)
    24: test	0.578	19.555	48.447 (5000)

    OPTIMIZED 7 PID levels
    24: test	0.526	18.080	44.386 (100)
    24: test	0.582	19.607	48.725 (5000)
    24: all worse than before

    REVERTED BACK TO TOP SCORE DATA
    24: test	0.572	19.376	47.983 (5000, RERUN of FINAL HIGH SCORE)

    NO INITIAL STEER:
    > 65: test	0.686	20.958	55.248 (100)
    > 60: test	0.673	20.822	54.452 (100)
    > 59: test	0.672	20.793	54.409 (100, BEST SCORE)
    > 58: test	0.673	20.818	54.458 (100)
    > 55: test	0.675	20.863	54.588 (100)
    > 50: test	0.687	21.051	55.405 (100)
    > 45: test	0.695	21.129	55.903 (100)

    EXPERIMENTAL PID + MODEL (step_idx filter)
    < 105: test	0.563	18.707	46.858 (100)
    < 106: test	0.561	18.735	46.807 (100)
    < 107: test	0.561	18.717	46.760 (100, BEST FILTER)
    < 107: test	0.612	20.164	50.785 (5000, 4th rank (close to 3rd))
    < 108: test	0.562	18.731	46.829 (100)
    < 109: test	0.564	18.759	46.946 (100)
    < 110: test	0.563	18.707	46.858 (100)
    < 115: test	0.564	18.714	46.926 (100)
    < 117: test	0.566	18.721	47.024 (100)
    < 120: test	0.568	18.756	47.141 (100)
    < 120: test	0.622	20.218	51.318 (5000, back to 4th rank =()

    EXPERIMENTAL PID + MODEL with lataccel diff
    < 107 + 0.035: test	0.564	18.699	46.899 (100)
    < 107 + 0.035: test	0.622	20.154	51.276 (5000)

    TOP PID
    < 105: test	0.568	18.830	47.245 (100)
    < 107: test	0.567	18.852	47.211 (100)
    < 109: test	0.567	18.851	47.195 (100)
    < 120: test	0.574	18.934	47.620 (100)

    EXPERIMENTAL + control signal diff
    < 103 + 0.025: test	0.564	18.698	46.874 (100)
    < 103 + 0.015: test	0.563	18.687	46.821 (100)
    < 103 + 0.0155: test	0.563	18.684	46.813 (100)
    < 103 + 0.0155 + EXPERIMENTAL RESET <= 81 + lat diff 0.0155: test	0.561	18.691	46.733 (100)
    < 103 + same as above: test	0.614	20.124	50.815 (5000)
    < 107 + same as above: test	0.565	18.716	46.957 (100)
    < 107 + same as above: test	0.578	18.697	47.579 (200)

    PID W FF:
    < 105: test	0.570	18.867	47.360 (100)
    < 107: test	0.570	18.901	47.404 (100)

    MADE ALL PIDS RESET <= 81 step
    < 103: test	0.562	18.698	46.802 (100)
    < 104: test	0.561	18.677	46.735 (100)
    < 104: test	0.612	20.145	50.729 (5000)
    < 105: test	0.562	18.699	46.821 (100)
    < 105: test	0.610	20.132	50.633 (5000, NEW HIGH SCORE: Rank #3)
    < 105 + 3/256:  test	0.576	18.699	47.505 (200)
    < 105 + no filter: test	0.572	18.694	47.298 (200)
    < 106: test	0.611	20.143	50.678 (5000)

    EXPERIMENTAL + lat accel diff
    < 100 + 0.025: test	0.576	18.722	47.528 (200)
    < 100 + 0.015: test	0.577	18.744	47.606 (200)
    < 100 + 0.035: test	0.575	18.720	47.468 (200)
    < 105 + none : test	0.572	18.694	47.298 (200)
    < 105 + 0.025: test	0.579	18.707	47.674 (200)
    < 105 + 0.025: test	0.579	18.706	47.668 (200)

    EXPERIMENTS WITH INITIAL STEERS on LEVEL  03214
    - no initial steer
    Average lataccel_cost:  3.235, average jerk_cost:  74.12, average total_cost:  235.8

    - PID + sigma + model
    Average lataccel_cost:  3.083, average jerk_cost:  72.78, average total_cost:  226.9

    - initial steer from tiny physics
    Average lataccel_cost:  2.968, average jerk_cost:  71.56, average total_cost:  220.0

    NEW CONTROLLERS:
    - model: Requires initial steer values
        24: test	0.531	18.056	44.607 (100)
        24: test	0.572	19.376	47.983 (5000, +3.376 - RANK #1)
    - pid_model: Does not require initial steer values, and instead uses PID to initialize the model
        24: test	0.562	18.699	46.821 (100)
        24: test	0.610	20.132	50.633 (5000, +3.812 vs 100, +2.65 vs 5000 model.py, RANK #3)

    CRAZY NEW IDEA: +9 inputs (nearby torques: -2,-1,0,1,2), +9 outputs: sim score diffs
    ALLOW the controller to choose the best predicted torque
    00002: Experimental PID: Average lataccel_cost:  1.306, average jerk_cost:  21.83, average total_cost:  87.12
    00002: Explore model: Average lataccel_cost:  1.156, average jerk_cost:   25.6, average total_cost:  83.38
    00001: Experimental PID: Average lataccel_cost: 0.4484, average jerk_cost:  8.193, average total_cost:  30.61
    00001: Explore model: Average lataccel_cost: 0.4094, average jerk_cost:  9.163, average total_cost:  29.63
    00000: Experimental PID: Average lataccel_cost: 0.8059, average jerk_cost:  31.26, average total_cost:  71.55
    00000: Explore model: Average lataccel_cost:  1.011, average jerk_cost:  37.05, average total_cost:  87.59

    good model: 00000.csv: Average lataccel_cost: 0.7244, average jerk_cost:  29.93, average total_cost:  66.15
    new  model: 00000.csv: Average lataccel_cost:  1.043, average jerk_cost:  28.07, average total_cost:  80.21

    2: test	0.742	22.274	59.350 (100)
    3: test	0.750	22.321	59.822 (100)

    test	0.774	22.683	61.396 (100, experimental PID
    test	0.804	24.068	64.258 (100, good model)

    model,00002: Average lataccel_cost: 0.8634, average jerk_cost:   22.9, average total_cost:  66.07 (no explore)
    model,00002: Average lataccel_cost: 0.8783, average jerk_cost:  24.67, average total_cost:  68.58
    exper,00002: Average lataccel_cost:  1.413, average jerk_cost:  24.57, average total_cost:  95.22

    NEW DEEP TRAINING (No Steer: trained on random chunks of 500 drives per epoch, with 3 training repeats)
    - Nearly matches the performance of the original model I am distilling
    - No Q learning yet (this is just testing the simulation and noise code)
    9: test	0.574	18.538	47.244 (100 tests)
    9: test	0.620	19.728	50.744 (5000 tests)

