    PREVIOUS GAME LEVELS (including 80 initial steers)
    45: test	0.996	14.617	64.393
    50: test	0.987	14.686	64.018
    
    UPDATED GAME LEVELS: 00001.npy - 00006.npy:
    50: test	1.021	14.587	65.616
    
    UPDATED GAME LEVELS: 00120 - 00122:
    35: test    1.096	14.406	69.182
    
    UPDATED GAME LEVELS: 00123, 00124, 00159, 00254:
    35: test	1.096	14.406	69.182
    
    UPDATED GAME LEVELS: 00322, 00356, 00432:  *** EXPLODES MODEL
    35: test	12.312	16.892	632.476
    
    UPDATED GAME LEVELS: Reverting 00432:
    50: test	1.191	15.099	74.656
    
    UPDATED GAME LEVELS: Deleting 00432:
    35: test	1.525	14.532	90.791
    
    13 NEW GAME LEVELS: (equal distribution)
    30: test	1.317	13.932	79.772
    
    15 NEW GAME LEVELS: (equal distribution) 
    30: test	1.180	14.704	73.711 - ignore initial steer [80:]
    35: test	1.168	14.356	72.776 - with 80 initial steer

    27 NEW GAME LEVELS: (equal distribution) 
    20: test	1.235	14.621	76.391 - ignore initial steer [80:]
    45: test	1.144	15.202	72.409 - with 80 initial steer

    35 NEW GAME LEVELS: (equal distribution) 
    25: test	1.136	14.812	71.619 - ignore initial steer [80:]
    45: test	1.218	14.391	75.297 - with 80 initial steer
    
    39 NEW GAME LEVELS: (equal distribution) + 1 PID level
    20: test	1.059	14.936	67.875
    
    41 NEW GAME LEVELS: (equal distribution) + Many PID level
    45: test	1.144	14.474	71.688
    40: test    1.116	14.393	70.216 (no changes - just train again)
    
    CHANGE NN from 64 to 80 (no changes in data)
    40: test	1.019	14.771	65.712
    
    53 NEW GAME LEVELS: 
    20: test	1.088	14.478	68.882
    
    58 NEW GAME LEVELS: (deleted 4744, 900+ score)
    30: test	0.945	15.242	62.485
    40: test	0.928	15.453	61.843
    40: test	1.125	16.390	72.652 (5000 eval)
    
    CHANGE NN from 80 to 128 (includes 4744)
    25: test	0.977	15.707	64.548
    
    CHANGE NN to 96 (deleted 4744)
    20: test	1.028	15.082	66.482
    
    CHANGE NN back to 80 (set random seed: 42), still 58 game levels
    15: test	1.367	15.115	83.454
    20: test	1.560	15.529	93.540
    25: test	1.227	15.676	77.039
    30: test	1.396	15.966	85.776
    35: test	1.681	16.217	100.249
    
    NO CHANGES (set random seed: 1979)
    25: test	1.267	15.420	78.775
    30: test	1.207	15.981	76.311

    NO CHANGES (set random seed: 2000)
    25: test	1.248	15.336	77.750
    50: test	1.198	15.828	75.706

    NO CHANGES (set random seed: 2002, changed LR=0.00006)
    26: test	1.000	14.941	64.925

    NO CHANGES (set random seed: 962, changed LR=0.00006)
    25: test	0.992	14.764	64.375

    REMOVED LEVELS: 04743, 08335, 03166 (bad high scores)
    30: test	1.081	14.498	68.549  # slower to train

    ADDED LEVELS (1,2,3, 120, 121, 122) = got worse
    20: test	1.001	14.612	64.677

    REVERTED BACK TO 58 LEVELS (tensor_data[60:]) = got worse
    24: test	1.001	14.715	64.781

    UPDATE MODEL TO 64 NN (tensor_data[80:]) = got worse
    24: test	1.060	14.515	67.525

    UPDATE MODEL TO 80 NN
    25: test	0.992	14.764	64.375

    UPDATED GAME LEVELS to 63
    21: test	0.997	14.486	64.346

    UPDATED GAME LEVELS to 82 (removing 14278, 14400, 12758) = got worse  (more bad files: 08335, 04743, 03166)
    18: test	1.220	14.029	75.007

    TRAINING all 84 levels (analyzing which files are the worst over 20 epochs) - 10 worst files
    File                 Mean Loss       Epoch Count
    ---------------------------------------------
    1   ./simulations/03166.pth 0.743433        20
    2   ./simulations/04743.pth 0.458917        20
    3   ./simulations/14278.pth 0.442126        20
    4   ./simulations/00140.pth 0.265490        20
    5   ./simulations/01611.pth 0.263488        20
    6   ./simulations/02355.pth 0.244197        20
    7   ./simulations/07283.pth 0.217067        20
    8   ./simulations/14400.pth 0.216369        20
    9   ./simulations/00437.pth 0.199559        20
    10  ./simulations/08335.pth 0.183868        20
    11  ./simulations/04358.pth 0.167655        20

    REMOVED top 11 worst training files (73 levels remain)
    33: test	1.425	14.962	86.223

    SWITCHED loss function to SmoothL1Loss()
    33: test	1.440	14.912	86.916

    REVERTED. GOT G29 WHEEL. FIRST 7 WHEEL LEVELS (no PID)
    65: test	1.296	14.917	79.711  # still improving (worst files are high scores)

    IMPROVED 7 LEVELS (WHEEL ONLY) - Trained longer, 100 epochs
    83: test	1.186	15.063	74.386

    10 LEVELS (WHEEL ONLY)
    85: test	1.200	14.417	74.416

    File                 Mean Loss       Epoch Count
    ---------------------------------------------
    1   ./simulations/00140.pth 0.189727        100
    2   ./simulations/00437.pth 0.150982        100
    3   ./simulations/00367.pth 0.060024        100
    4   ./simulations/00669.pth 0.057937        100

    11 LEVELS (WHEEL ONLY) - updated 2 worst scores (got worse)
    61: test	1.255	13.962	76.721

    File                 Mean Loss       Epoch Count
    ---------------------------------------------
    1   ./simulations/00140.pth 0.168790        100
    2   ./simulations/00437.pth 0.145453        100
    3   ./simulations/00669.pth 0.062521        100
    4   ./simulations/00367.pth 0.060716        100

    11 LEVELS (WHEEL ONLY) - adding r2, slope to model (median with moving window) - model becomes very unstable after 75 epochs
    74: test	1.360	14.607	82.630

    27 LEVELS (WHEEL ONLY)
    23: test	1.045	14.524	66.753

    38 LEVELS (WHEEL ONLY)
    5: test	1.261	14.106	77.170

    File                 Mean Loss       Epoch Count
    ---------------------------------------------
    1   ./simulations/00140.pth 0.064160        100
    2   ./simulations/02355.pth 0.064128        100
    3   ./simulations/00437.pth 0.057320        100
    4   ./simulations/03273.pth 0.039570        100
    5   ./simulations/04358.pth 0.030827        100
    6   ./simulations/05216.pth 0.027430        100
    7   ./simulations/00367.pth 0.027366        100
    8   ./simulations/00669.pth 0.024083        100
    9   ./simulations/00236.pth 0.021043        100
    10  ./simulations/03975.pth 0.020578        100

    38 LEVELS (NO CHANGES) - lowering LR
    15: test	1.118	14.319	70.242

    File                 Mean Loss       Epoch Count
    ---------------------------------------------
    1   ./simulations/02355.pth 0.272035        40
    2   ./simulations/00140.pth 0.234221        40
    3   ./simulations/00437.pth 0.220579        40
    4   ./simulations/04358.pth 0.122256        40
    5   ./simulations/03273.pth 0.120247        40
    6   ./simulations/05679.pth 0.089220        40
    7   ./simulations/05216.pth 0.083075        40
    8   ./simulations/00367.pth 0.082475        40
    9   ./simulations/00669.pth 0.082144        40
    10  ./simulations/05324.pth 0.076274        40

    NO CHANGES - Changing NN to 96 AND 64 params
    X: BOTH BAD (96 worst performance, 64 also bad, 75 a little bad)
    40: test	1.077	15.159	69.033 - NN to 85

    NO CHANGES - NN to 85, input size = 25 (6 future values for all 4 categories) + previous action
    X: test	1.122	14.514	70.614

    Removed level 02355 (worst training loss)
    X: Got a little worse

    Added levels 51, 69 (worst scores in top 100)
    X: test	1.114	14.449	70.138

    ADDED DROPOUT (learning slower, more gradual, not as good scores)
    37: test	1.100	14.683	69.700 = dropout 0.3
    X:   = dropout 0.4 = identical to 0.3 dropout

    NO DROPOUT (lr=5.1039484000888e-05, seed=468075), Removed level 51, 29
    12: test	1.111	14.349	69.875
    12: test	1.368	14.559	82.937 (5000 tests)

    UPDATED scores on 5 levels (using arrows / road roll in game)
    12: test	1.116	14.418	70.207
    15: test	1.116	14.459	70.280  # update 2 more levels scores
    1   ./simulations/02355.pth 0.414758        25
    2   ./simulations/00140.pth 0.371897        25
    3   ./simulations/00437.pth 0.338049        25
    4   ./simulations/04358.pth 0.189470        25
    5   ./simulations/03273.pth 0.173253        25
    6   ./simulations/05679.pth 0.135399        25

    ADDED 8 new levels (new levels lowered score)
    25: test	0.981	15.242	64.276 (100 levels)
    30: test	0.950	15.312	62.812 (100 levels)
    30: test	1.092	16.669	71.246 (5000 levels)

    IMPROVED 10 high scores
    34: test	0.976	15.593	64.404 (100 levels, window size 22)
    37: test	0.951	15.717	63.261 (window size 30)
    37: test	0.947	15.689	63.030 (window size 20)

    REMOVING level 140 + ignoring windows > diff_threshold
    30: test	1.185	14.460	73.693 (diff .085, window size 20)
    30: test	0.942	15.541	62.646 (diff .1, window size 20)

    NO DIFFS - removed levels
    - removed 140 - got tiny bit better
    - removed 437 - got worse (added back)
    - removed 2355 - got tiny bit better: test	0.944	15.572	62.790
    - removed 7283 - got much worse (added back)
    - removed 6828 - got much worse (added back)
    - removed 7614 - got much worse (added back)
    - removed 4358 - got little worse (added back)

    1   ./simulations/00437.pth 0.098631        65
    2   ./simulations/07283.pth 0.092984        65
    3   ./simulations/06828.pth 0.070252        65
    4   ./simulations/07614.pth 0.065340        65
    5   ./simulations/04358.pth 0.060257        65
    6   ./simulations/03273.pth 0.054075        65
    7   ./simulations/07682.pth 0.049648        65

    DIFFS forward and backwards (0.03 to 1.0, 1.0 to 0.03)
    - No improvement
    - Linear (1.0 to 0.03), lr=0.000055: worse
    - Sigmoid curve (1.0 to 0.03), lr=0.000055, removed 8027: test	0.968	15.465	63.871
    - Sigmoid curve (0.03 to 1.0), lr=0.000055, removed 8027: test	0.968	15.465	63.871
    - No DIFF, lr=0.000055, removed 8027: test	0.969	15.284	63.718

    NO DIFFS, Adding 3 new files (1 by 1), updated 7682
    - Updated 7682: got much worse
    - Removed 7682: smoother cost curve, but a little worse (reverting back to un-updated version)
    - Added 7724, 7922, and 8027: Nice cost curve, but got a little worse
    - Added just 7724, 7922: Nice cost curve, but little worse: test	0.978	15.264	64.160

    CHANGE OPTIMIZER to AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)
    - terrible / broken

    SIMULATIONS
    - Random 20 files (seed 1979): 47: test	0.972	14.976	63.593
    - Update lr=5e-04: test	0.949	15.636	63.109
    - ONLY files 20% better than PID scores: test	0.968	15.188	63.603
    - ONLY files 15% better than PID scores: test	0.975	15.392	64.165
    - ONLY files 10% better than PID scores: test	0.923	15.556	61.714
    - ONLY files 5% better than PID scores: test	0.949	15.345	62.802

    ADDED 10 new levels
    - 60: test	0.956	15.765	63.576
    - Removed 08335: got worse

    File                 Mean Loss       Epoch Count
    ---------------------------------------------
    1   ./simulations/08335.pth 0.180724        60
    2   ./simulations/00437.pth 0.138862        60
    3   ./simulations/07283.pth 0.124840        60
    4   ./simulations/06828.pth 0.105372        60

    ADDED 7 MODEL OUTPUTS (STEER + 6 FUTURE DIFFS)
    25: test	0.916	16.920	62.724 (100 tests, but 5000 test score sucks: ~80)
    25: test	0.861	18.052	61.123 (added small torque adj in pid_model, based on diffs)
    25: test	0.823	18.036	59.177 (made torque adj dynamic: kP=.152)
    25: test	1.108	20.750	76.160 (approach did not scale to 5000 tests for some reason)
    25: test	1.100	19.067	74.062 (kP=.09, still didn't scale to 5000 tests)

    UPDATED MODEL NN to 100
    24: test	0.925	16.279	62.505 (torque adj dynamic: kP=.09)
    24: test	1.441	17.665	89.739 (5000 tests, bad score)

    OPTIMIZER for LEVEL DATA
    00002: Improved total cost from 95.93814630552095 to 72.61961958909242 (3 indexes in each direction, 1 iteration)
    00002: Improved total cost from 95.93814630552095 to 70.719230186088 (4 indexes in each direction, 1 iteration)
    Starting full optimizer run: 1:25 PM (34 minutes per file x 41 files == 25 hours)
    34: test	1.216	14.977	75.755 (8 optimized files, 100 tests, lr=0.0001)
    54: test	0.919	16.493	62.467 (25 optimized files, 100 tests, lr=0.0001)
    29: test	0.967	14.894	63.265 (31 optimized files, 100 tests, lr=0.00005)
    X:  (56 optimized files, 100 tests, lr=0.000085 - unstable LR)
    31: test	0.998	14.741	64.629 (56 optimized files, 100 tests, lr=0.00004)
    57: test	0.936	14.960	61.775 (56 optimized files, 100 tests, lr=0.00002)

    ADDING WORST LEVELS (in 100 tests)
    00033 00026, 00069, 00051, 00089
    40: test	0.931	14.650	61.205 (63 optimized files, lr=0.00002)
    80: test	0.910	14.772	60.295 (63 optimized files, lr=0.00001 - slow LR)
    60: test	0.931	14.730	61.291 (63 optimized files, lr=0.000015, 2nd optimization of lvl 26)

    UPDATE NN to 100, 128
    90: test	0.929	15.032	61.457 (NN 100, lr=0.00001 - slow to train)
    82: test	0.931	14.928	61.476 (NN 100, lr=0.0000125 - no real improvement)

    REOPTIMIZE 8335, 7283, NN back to 85 size, lr=00001
    100: test	0.921	14.918	60.980 (lr=0.00001 - slow to train)
    100: test	1.084	15.456	69.654 (5000 tests - best score yet)

    4 WORST SCORES (5000 tests): 01805.csv, 00633.csv, 04744.csv, 01716.csv, 00432.csv
    PLUS: 03001.csv solved with pid_top PID. ALL 6 new levels optimized multiple times.
    PLUS: Less future ranges: [(0, 1), (1, 3), (2, 5), (5, 9)], remove a_ego segments - 13 INPUTS
    X: test	0.846	14.877	57.163 (100 tests, lr=0.00001)
    X: test	0.938	15.586	62.500 (5000 tests, lr=0.00001 - NEW HIGH SCORE!!!!! +5.337)
    X: (lr=0.00001, added lataccel w/roll adjustment, back to 17 inputs - no improvement)
    53: test	0.794	15.589	55.293 (100 tests, lr=0.00001, less future ranges: [(0, 1), (1, 3), (2, 5)], +3 prev actions = 15 inputs)
    53: test	0.864	16.629	59.824 (5000 tests - NEW HIGH SCORE!!!!! +4.531)
    59: test	0.786	15.502	54.824 (100 tests)
    60: test	0.784	15.496	54.703 (100 tests)
    60: test	0.783	15.484	54.657 (100 tests, 34 window size)
    60: test	0.864	16.604	59.781 (5000 tests, 34 window size)

    MORE OPTIMIZED SIMULATIONS
    57: (14 optimized, 100 tests)
    57: (14 optimized, 5000 tests)

    MADE v_ego and a_ego not subtract current state, removed 04744
    - test	0.762	15.839	53.941 (100 tests)
    - test	0.853	16.708	59.366 (5000 tests, NEW HIGH SCORE!!!!! +5.425)

    OPTIMIZED 38 total files
    50: test	0.766	15.745	54.052 (100 tests - tiny improvement on 100 tests)

    PLOTTED MISSING LEVELS: 1883, 4874, 3277, 2546, 367, 1459, 3533, 1830, 2429, 4853, 739, 3281, 3727, 3546, 740, 2684, 1961, 319, 1793, 3873, 1654, 575, 387, 4649, 1487
    46: test	0.805	15.426	55.677 (10 new levels, 100 tests, a little worse on all tests)

    COMBINED SIMULATIONS (44 PID-REPLAY + 36 PID-TOP)
    45: test	0.693	16.309	50.954 (100 tests)
    46: test	0.690	16.319	50.830 (100 tests)
    46: test	0.747	17.737	55.089 (5000 tests, NEW HIGH SCORE!!!!! +4.259)

    MADE v_ego and a_ego not subtract current state (a small improvement)
    48: test	0.679	16.426	50.352 (100 tests)
    48: Average Cost: 50.7170 (200 tests - ran test_models on 200 tests)
    48: test	0.758	17.927	55.849 (5000 tests, +5.497)
    43: test	0.761	17.995	56.049 (5000 tests, lowest test score in 20 tests)

    MADE only v_ego absolute (reverted a_ego). ADDED PID-FF. PID-TOP: 24 wins, PID-REPLAY: 39 wins, PID-FF: 17 wins
    X: test	0.639	16.724	48.665 (100 tests, big improvement)
    X: test	0.722	18.520	54.641 (5000 tests, NEW HIGH SCORE!!!! +5.975)

    ADDED A FEW MISSING PLOT LEVELS: PID-TOP: 36 wins, PID-REPLAY: 40 wins, PID-FF: 24 wins
    32: test	0.619	17.150	48.092 (100 tests)
    33: test	0.619	17.107	48.048 (100 tests)

    ADDED 20 HIGHEST SCORES FROM CSV (PID-TOP: 40 wins, PID-REPLAY: 42 wins, PID-FF: 37 wins, PID: 1 wins), W/ 0.08 filtering
    45: test	0.630	17.083	48.559 (100 tests, test score plots slightly worse)
    45: test	0.682	18.372	52.459 (5000 tests, NEW HIGH SCORE!!! +3.9)
    60: test	0.633	17.055	48.705 (0.07 filtering, lower LR: 0.000009, 100 tests, slightly worse)
    42: test	0.624	17.101	48.303 (0.09 filtering, 100 tests, slightly better)
    42: test	0.652	17.717	50.301 (300 tests)
    50: test	0.699	17.930	52.862 (300 tests)
    42: test	0.689	18.688	53.142 (0.09 filtering, 5000 tests)

    UPDATING SAVE SIMULATIONS (ALSO SAVE CLOSE REPLAYS)
    WORST SCORES: 4959, 4744, 3166, 1722, 2531, 1193, 4803
    - 04959: PID : 2502.9 cost (PID-REPLAY: +2427.4, PID-TOP: +899.3, PID-FF: +1455.7, PID-FUTURE: +11481.2)
    - 04744: PID-REPLAY : 668.3 cost (PID-TOP: +1085.9, PID-FF: +4904.8, PID-FUTURE: +53361.2, PID: +3889.9)
    - 03166: PID-TOP : 565.7 cost (PID-REPLAY: +20787.1, PID-FF: +114.7, PID-FUTURE: +5845.3, PID: +1279.2)
    - 01722: PID-FF : 640.5 cost (PID-REPLAY: +11295.4, PID-TOP: +160.6, PID-FUTURE: +17406.9, PID: +2084.7)
    - 02531: PID-FF : 785.9 cost (PID-REPLAY: +13442.0, PID-TOP: +246.0, PID-FUTURE: +88002.3, PID: +3143.2)
    - 01193: PID-FF : 410.5 cost (PID-REPLAY: +9961.2, PID-TOP: +224.7, PID-FUTURE: +56317.8, PID: +1623.6)
    - 04803: PID-FF : 541.4 cost (PID-REPLAY: +8318.7, PID-TOP: +168.9, PID-FUTURE: +5655.7, PID: +1705.8)
    TOTAL LEVELS: 138 SIMULATIONS: PID-TOP: 39 wins, PID-REPLAY: 80 wins, PID-FF: 33 wins
    43: test	0.639	17.045	48.977 (100 tests, 0.075 filter)
    43: test	0.687	18.369	52.708 (5000 tests)
    24: test	0.642	16.754	48.865 (100 tests, no filter)
    24: test	0.743	18.676	55.819 (5000 tests, no filter, also with close replays)

    NO CLOSE REPLAYS (PID-TOP: 39 wins, PID-REPLAY: 54 wins, PID-FF: 33 wins)
    X: test	0.626	16.937	48.217 (100 tests, NO FILTER)
    X: test	0.706	18.602	53.882 (5000 tests, NO FILTER)

    ADDED COST TO EACH STEERING TORQUE (train on all data, but weight it based on cost)
    31: test	0.633	16.884	48.537 (100 tests, ABS + Clamp steer cost -1:1)
    31: test	0.706	18.480	53.765 (5000 tests, ABS + Clamp steer cost -1:1)
    29: test	0.625	16.890	48.118 (100 tests, Clamp -6:6 => 1 to 0)
    29: test	0.626	16.882	48.159 (100 tests, Clamp -2.5:2.5 => 1 to 0, smoother validation curve, slight improvement)
    29: test	0.626	16.865	48.154 (100 tests, Clamp -2.5:2.5 => 1 to 0, plus sigmoid)
    27: test	0.622	16.922	48.002 (100 tests, Clamp -2.5:0.5 => 1 to 0, plus sigmoid)
    27: test	0.623	17.292	48.419 (200 tests)
    27: test	0.698	18.509	53.407 (5000 tests)
    22: test	0.643	16.663	48.813 (100 tests, Clamp -2:0 => 1 to 0.5, no sigmoid, L1Loss)
    22: test	0.720	18.310	54.305 (5000 tests, Clamp -2:0 => 1 to 0.5, no sigmoid, L1Loss)
    X: (120 epoch training - unstable validation curve after epoch 80)

    ADDING DROPOUT to model (trying to address overfitting)
    34: test	0.688	16.179	50.598 (100 tests, dropout 0.2, no LSTM dropout, MSELoss, NN 64, unstable validation)
    X: (NN 80, dropout 0.4, no LSTM dropout, unstable validation)
    32: test	0.627	16.708	48.069 (100 tests, NN 80, dropout 0.0, no LSTM dropout)
    32: test	0.625	16.846	48.109 (200 tests)
    32: test	0.685	18.061	52.322 (5000 tests, NEW HIGH SCORE!!!!! +4.253)
    41: test	0.669	16.168	49.615 (100 tests, NN 80, dropout 0.25, no LSTM dropout - little worse)
    X: (100 tests, NN 80, dropout 0.35, no LSTM dropout - even worse)
    35: test	0.644	16.495	48.706 (100 tests, NN 80, dropout 0.1, no LSTM dropout - the less dropout, the better the score)
    35: test	0.641	16.618	48.665 (200 tests)
    35: test	0.710	17.885	53.406 (5000 tests)